{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.495376Z",
     "start_time": "2025-04-01T16:27:43.584345Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 22:06:12.895822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743545172.913862  574073 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743545172.919402  574073 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743545172.932861  574073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743545172.932886  574073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743545172.932888  574073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743545172.932890  574073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-01 22:06:12.937351: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to sys.path\n",
    "parent_dir = Path(\"..\").resolve()\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import importlib\n",
    "import common_utils\n",
    "importlib.reload(common_utils)\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "from common_utils import (\n",
    "    get_unique_image_shapes,\n",
    "    get_unique_image_paths,\n",
    "    load_images_from_paths,\n",
    "    build_image_dataframe,\n",
    "    split_data,\n",
    "    bin_ages,\n",
    "    build_cnn_model,\n",
    "    build_model_from_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b449a3fa11f15cff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.505672Z",
     "start_time": "2025-04-01T16:27:47.496286Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5acada596262b76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.516108Z",
     "start_time": "2025-04-01T16:27:47.506352Z"
    }
   },
   "outputs": [],
   "source": [
    "image_paths_csv = pd.read_csv('./processed_data/image_paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6164dda010f022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.519887Z",
     "start_time": "2025-04-01T16:27:47.517146Z"
    }
   },
   "outputs": [],
   "source": [
    "paths_train_df, paths_val_df, paths_test_df = split_data(image_paths_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c9e2ceb3a1f9176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.527273Z",
     "start_time": "2025-04-01T16:27:47.520343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>age</th>\n",
       "      <th>age_bin</th>\n",
       "      <th>age_bin_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../raw_data2/face_age/018/4734.png</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>Young Adults (18–24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../raw_data2/face_age/007/6755.png</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Children (6–8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../raw_data2/face_age/024/3507.png</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>Young Adults (18–24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../raw_data2/face_age/068/1399.png</td>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>Seniors (65–74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../raw_data2/face_age/062/4152.png</td>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>Older Adults (55–64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>../raw_data2/face_age/016/4636.png</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>Teens (13–17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888</th>\n",
       "      <td>../raw_data2/face_age/003/5199.png</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Toddlers (3–5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6889</th>\n",
       "      <td>../raw_data2/face_age/007/6535.png</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Children (6–8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>../raw_data2/face_age/038/6094.png</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>Mid Adults (35–44)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6891</th>\n",
       "      <td>../raw_data2/face_age/060/9175.png</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>Older Adults (55–64)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6892 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    path  age  age_bin         age_bin_label\n",
       "0     ../raw_data2/face_age/018/4734.png   18        5  Young Adults (18–24)\n",
       "1     ../raw_data2/face_age/007/6755.png    7        2        Children (6–8)\n",
       "2     ../raw_data2/face_age/024/3507.png   24        5  Young Adults (18–24)\n",
       "3     ../raw_data2/face_age/068/1399.png   68       10       Seniors (65–74)\n",
       "4     ../raw_data2/face_age/062/4152.png   62        9  Older Adults (55–64)\n",
       "...                                  ...  ...      ...                   ...\n",
       "6887  ../raw_data2/face_age/016/4636.png   16        4         Teens (13–17)\n",
       "6888  ../raw_data2/face_age/003/5199.png    3        1        Toddlers (3–5)\n",
       "6889  ../raw_data2/face_age/007/6535.png    7        2        Children (6–8)\n",
       "6890  ../raw_data2/face_age/038/6094.png   38        7    Mid Adults (35–44)\n",
       "6891  ../raw_data2/face_age/060/9175.png   60        9  Older Adults (55–64)\n",
       "\n",
       "[6892 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e398f76e04c83c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.544555Z",
     "start_time": "2025-04-01T16:27:47.527940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743545175.728228  574073 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43622 MB memory:  -> device: 0, name: NVIDIA L40S, pci bus id: 0000:81:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Converting the filenames and target class labels into lists for augmented train and test datasets.\n",
    "\n",
    "train_filenames_list = list(paths_train_df['path'])\n",
    "train_labels_list = list(paths_train_df['age_bin'])\n",
    "\n",
    "train_filenames_tensor = tf.constant(train_filenames_list)\n",
    "train_labels_tensor = tf.constant(train_labels_list)\n",
    "\n",
    "val_filenames_list = list(paths_val_df['path'])\n",
    "val_labels_list = list(paths_val_df['age_bin'])\n",
    "\n",
    "val_filenames_tensor = tf.constant(val_filenames_list)\n",
    "val_labels_tensor = tf.constant(val_labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "581a5c16afb90f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.546315Z",
     "start_time": "2025-04-01T16:27:47.545146Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f77f27c3567505cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.548619Z",
     "start_time": "2025-04-01T16:27:47.546764Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "configurations = {\n",
    "    'channels': [1,3],\n",
    "    'num_conv_layers': [3,4],\n",
    "    'base_filters': [32,64],\n",
    "    'kernel_size': [3,5],\n",
    "    'activation': ['relu','swish'],\n",
    "    'use_skip': [False,True],\n",
    "    'num_dense_layers': [1,2],\n",
    "    'dense_units': [128,256],\n",
    "    'num_classes': [13],\n",
    "    'dropout_rate': [0.3, 0.5],\n",
    "    'output_activation': ['softmax','sigmoid'],\n",
    "    'pool_size': [2],\n",
    "    'task': ['classification']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca3121863a1d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e736c21984667af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.550831Z",
     "start_time": "2025-04-01T16:27:47.549137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of combinations : %d 1024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_combinations = list(itertools.product(*configurations.values()))\n",
    "print(f\"nr of combinations : %d\", len(all_combinations))\n",
    "valid_configs = []\n",
    "\n",
    "for combo in all_combinations:\n",
    "    params = dict(zip(configurations.keys(), combo))\n",
    "    \n",
    "    # Enforce: If use_skip=True, use_pooling=False\n",
    "    if params['use_skip']:\n",
    "        params['use_pooling'] = False\n",
    "    else:\n",
    "        params['use_pooling'] = True  # Or add to search space\n",
    "    \n",
    "    valid_configs.append(params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c148d5e8b1c8ddb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T16:27:47.595752Z",
     "start_time": "2025-04-01T16:27:47.553874Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = load_images_from_paths(train_filenames_tensor,train_labels_tensor, channels=3,ratio=1, batch_size=batch_size)\n",
    "val_dataset = load_images_from_paths(val_filenames_tensor,val_labels_tensor, channels=3,ratio=1,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3b6c2badd58277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-01 22:06:15,889] A new study created in memory with name: no-name-6cbee8aa-368c-41e6-83f8-bef6a3802869\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743545178.450930  575491 service.cc:152] XLA service 0x7f265400d910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743545178.450978  575491 service.cc:160]   StreamExecutor device (0): NVIDIA L40S, Compute Capability 8.9\n",
      "2025-04-01 22:06:18.572305: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743545178.825835  575491 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-04-01 22:06:23.351438: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2366', 12 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-04-01 22:06:23.431511: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2366', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-04-01 22:06:23.646787: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2438', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-04-01 22:06:23.662307: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2438', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2025-04-01 22:06:23.663261: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2438', 36 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-04-01 22:06:23.732032: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_688', 188 bytes spill stores, 188 bytes spill loads\n",
      "\n",
      "2025-04-01 22:06:29.461847: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.10 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,200,200]{3,2,1,0} %bitcast.11824, f32[256,128,7,7]{3,2,1,0} %bitcast.11808, f32[256]{0} %bitcast.11845), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:31.541242: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.079535178s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.10 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,128,200,200]{3,2,1,0} %bitcast.11824, f32[256,128,7,7]{3,2,1,0} %bitcast.11808, f32[256]{0} %bitcast.11845), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:33.830143: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng33{k2=15,k6=2,k13=0,k14=0,k22=0} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:34.205727: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.37565986s\n",
      "Trying algorithm eng33{k2=15,k6=2,k13=0,k14=0,k22=0} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:35.205938: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:35.680170: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.474367927s\n",
      "Trying algorithm eng20{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:36.680352: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng33{k2=2,k6=0,k13=2,k14=0,k22=2} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:37.251744: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.571521158s\n",
      "Trying algorithm eng33{k2=2,k6=0,k13=2,k14=0,k22=2} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:38.251929: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=1,k3=0} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:38.701626: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.449834552s\n",
      "Trying algorithm eng11{k2=1,k3=0} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:39.701738: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=0,k3=0} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:40.407667: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.705999301s\n",
      "Trying algorithm eng11{k2=0,k3=0} for conv %cudnn-conv-bias-activation.11 = (f32[32,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[512,256,7,7]{3,2,1,0} %bitcast.11947, f32[512]{0} %bitcast.11984), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:46.053656: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng26{k2=0,k4=2,k5=1,k6=0,k7=0,k19=1} for conv %cudnn-conv-bw-input.2 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,200,200]{3,2,1,0} %bitcast.11995, f32[512,256,7,7]{3,2,1,0} %bitcast.11947), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:46.423880: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.370292805s\n",
      "Trying algorithm eng26{k2=0,k4=2,k5=1,k6=0,k7=0,k19=1} for conv %cudnn-conv-bw-input.2 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,200,200]{3,2,1,0} %bitcast.11995, f32[512,256,7,7]{3,2,1,0} %bitcast.11947), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:47.713881: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng2{k2=0,k3=0} for conv %cudnn-conv-bw-input.2 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,200,200]{3,2,1,0} %bitcast.11995, f32[512,256,7,7]{3,2,1,0} %bitcast.11947), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:49.962533: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.248703801s\n",
      "Trying algorithm eng2{k2=0,k3=0} for conv %cudnn-conv-bw-input.2 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,200,200]{3,2,1,0} %bitcast.11995, f32[512,256,7,7]{3,2,1,0} %bitcast.11947), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:50.963255: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng25{k2=0,k3=0} for conv %cudnn-conv-bw-input.2 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,200,200]{3,2,1,0} %bitcast.11995, f32[512,256,7,7]{3,2,1,0} %bitcast.11947), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:51.502834: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.540159372s\n",
      "Trying algorithm eng25{k2=0,k3=0} for conv %cudnn-conv-bw-input.2 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,200,200]{3,2,1,0} %bitcast.11995, f32[512,256,7,7]{3,2,1,0} %bitcast.11947), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:52.502996: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng2{k2=1,k3=0} for conv %cudnn-conv-bw-input.2 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,200,200]{3,2,1,0} %bitcast.11995, f32[512,256,7,7]{3,2,1,0} %bitcast.11947), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:53.855819: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.352878332s\n",
      "Trying algorithm eng2{k2=1,k3=0} for conv %cudnn-conv-bw-input.2 = (f32[32,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,512,200,200]{3,2,1,0} %bitcast.11995, f32[512,256,7,7]{3,2,1,0} %bitcast.11947), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:57.565352: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng2{k2=0,k3=0} for conv %cudnn-conv-bw-input.3 = (f32[32,128,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11856, f32[256,128,7,7]{3,2,1,0} %bitcast.11808), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:06:57.570952: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.005715282s\n",
      "Trying algorithm eng2{k2=0,k3=0} for conv %cudnn-conv-bw-input.3 = (f32[32,128,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11856, f32[256,128,7,7]{3,2,1,0} %bitcast.11808), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", metadata={op_type=\"Conv2DBackpropInput\" op_name=\"gradient_tape/functional_1/conv2d_1_2/convolution/Conv2DBackpropInput\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:09.863002: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng1{k2=1,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:10.504258: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.641340451s\n",
      "Trying algorithm eng1{k2=1,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:11.504448: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=1,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:12.166219: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.661901581s\n",
      "Trying algorithm eng20{k2=1,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:13.166332: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=5,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:14.104298: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.938024599s\n",
      "Trying algorithm eng20{k2=5,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:15.104533: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=8,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:16.409521: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.305108309s\n",
      "Trying algorithm eng20{k2=8,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:17.409672: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng1{k2=6,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:19.262179: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.852591082s\n",
      "Trying algorithm eng1{k2=6,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:20.262433: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=6,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:07:21.910852: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.648556916s\n",
      "Trying algorithm eng20{k2=6,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,256,200,200]{3,2,1,0} %bitcast.11963, f32[32,512,200,200]{3,2,1,0} %bitcast.11995), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "I0000 00:00:1743545245.222856  575491 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-04-01 22:09:29.893891: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_688', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-04-01 22:09:30.060182: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_688', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-01 22:09:33.018585: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.10 = (f32[12,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,128,200,200]{3,2,1,0} %bitcast.11848, f32[256,128,7,7]{3,2,1,0} %bitcast.11832, f32[256]{0} %bitcast.11869), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:09:33.227321: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.208836089s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.10 = (f32[12,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,128,200,200]{3,2,1,0} %bitcast.11848, f32[256,128,7,7]{3,2,1,0} %bitcast.11832, f32[256]{0} %bitcast.11869), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:09:54.034564: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng1{k2=6,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,256,200,200]{3,2,1,0} %bitcast.11987, f32[12,512,200,200]{3,2,1,0} %bitcast.12019), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:09:54.112354: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.07788181s\n",
      "Trying algorithm eng1{k2=6,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,256,200,200]{3,2,1,0} %bitcast.11987, f32[12,512,200,200]{3,2,1,0} %bitcast.12019), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:09:55.112511: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=6,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,256,200,200]{3,2,1,0} %bitcast.11987, f32[12,512,200,200]{3,2,1,0} %bitcast.12019), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:09:55.125541: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.013122972s\n",
      "Trying algorithm eng20{k2=6,k3=0} for conv %cudnn-conv-bw-filter.5 = (f32[512,256,7,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[12,256,200,200]{3,2,1,0} %bitcast.11987, f32[12,512,200,200]{3,2,1,0} %bitcast.12019), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_2_1/convolution/Conv2DBackpropFilter\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:09:55.125617: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 65.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-01 22:10:09.087356: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_152', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-04-01 22:10:09.426175: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_152', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n",
      "2025-04-01 22:10:13.864392: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.10 = (f32[27,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,128,200,200]{3,2,1,0} %bitcast.885, f32[256,128,7,7]{3,2,1,0} %bitcast.892, f32[256]{0} %bitcast.894), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:15.574041: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 2.70977569s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.10 = (f32[27,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,128,200,200]{3,2,1,0} %bitcast.885, f32[256,128,7,7]{3,2,1,0} %bitcast.892, f32[256]{0} %bitcast.894), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_1_2/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:17.645898: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng33{k2=15,k6=2,k13=0,k14=0,k22=0} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:17.843429: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.19764286s\n",
      "Trying algorithm eng33{k2=15,k6=2,k13=0,k14=0,k22=0} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:18.843570: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:19.142767: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.299286054s\n",
      "Trying algorithm eng20{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:20.142954: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng33{k2=2,k6=0,k13=2,k14=0,k22=2} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:20.493813: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.350950272s\n",
      "Trying algorithm eng33{k2=2,k6=0,k13=2,k14=0,k22=2} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:21.493944: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=1,k3=0} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:21.756186: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.262333518s\n",
      "Trying algorithm eng11{k2=1,k3=0} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:22.756296: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=0,k3=0} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:10:23.250402: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.494175782s\n",
      "Trying algorithm eng11{k2=0,k3=0} for conv %cudnn-conv-bias-activation.11 = (f32[27,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[27,256,200,200]{3,2,1,0} %bitcast.934, f32[512,256,7,7]{3,2,1,0} %bitcast.941, f32[512]{0} %bitcast.943), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1/conv2d_2_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "[I 2025-04-01 22:37:55,241] Trial 0 finished with value: 0.2019733041524887 and parameters: {'batch_size': 32, 'channels': 1, 'num_conv_layers': 3, 'base_filters': 128, 'kernel_size': 7, 'activation': 'swish', 'num_dense_layers': 3, 'dense_units': 256, 'dropout_rate': 0.3, 'output_activation': 'softmax', 'pool_size': 3}. Best is trial 0 with value: 0.2019733041524887.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 | Model: 1ch_skip_False_conv3_k7_swish_dense3x256_drop0.3_out_softmax | Val Acc: 0.2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 22:38:03.628282: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1738_0', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.690501: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_888', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.698693: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1810', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.778282: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1810', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.780256: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_888', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.783612: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1810_0', 456 bytes spill stores, 456 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.785538: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1738', 116 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.845354: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1810', 280 bytes spill stores, 280 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.879200: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_888', 468 bytes spill stores, 468 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.895019: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_888', 120 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.899528: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_888', 584 bytes spill stores, 584 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.916854: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1738', 220 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.942378: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1810', 272 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:03.964930: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1738', 116 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:04.046065: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_888', 2936 bytes spill stores, 2940 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:04.216576: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_888_0', 692 bytes spill stores, 2152 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:04.794334: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_888_0', 2420 bytes spill stores, 4184 bytes spill loads\n",
      "\n",
      "2025-04-01 22:38:07.942528: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 11.98GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-01 22:38:07.942593: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 11.98GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-01 22:38:08.942763: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng20{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:11.083711: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.141090309s\n",
      "Trying algorithm eng20{k2=1,k4=2,k5=1,k6=0,k7=0,k19=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:12.083870: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=0,k3=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:14.642890: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.559126369s\n",
      "Trying algorithm eng11{k2=0,k3=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:14.642944: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 11.98GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-01 22:38:15.643063: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=1,k3=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:17.694233: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 3.051263813s\n",
      "Trying algorithm eng11{k2=1,k3=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:18.694378: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=3,k3=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:23.231673: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 5.537381702s\n",
      "Trying algorithm eng11{k2=3,k3=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:24.231868: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng11{k2=4,k3=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:28.967357: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 5.73554428s\n",
      "Trying algorithm eng11{k2=4,k3=0} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:29.967634: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:54.674726: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 25.707186843s\n",
      "Trying algorithm eng0{} for conv %cudnn-conv-bias-activation.13 = (f32[256,256,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,128,200,200]{3,2,1,0} %bitcast.7315, f32[256,128,7,7]{3,2,1,0} %bitcast.7299, f32[256]{0} %bitcast.7336), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_4_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 22:38:54.676727: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 19.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-04-01 22:38:54.776711: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at xla_ops.cc:591 : UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n",
      "%cudnn-conv-bias-activation.14 = (f32[256,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,256,200,200]{3,2,1,0} %bitcast.7454, f32[512,256,7,7]{3,2,1,0} %bitcast.7438, f32[512]{0} %bitcast.7475), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_5_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "\n",
      "Original error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 20988297216 bytes. [tf-allocator-allocation-error='']\n",
      "\n",
      "To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n",
      "2025-04-01 22:38:54.777091: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: Failed to determine best cudnn convolution algorithm for:\n",
      "%cudnn-conv-bias-activation.14 = (f32[256,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,256,200,200]{3,2,1,0} %bitcast.7454, f32[512,256,7,7]{3,2,1,0} %bitcast.7438, f32[512]{0} %bitcast.7475), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_5_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "\n",
      "Original error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 20988297216 bytes. [tf-allocator-allocation-error='']\n",
      "\n",
      "To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n",
      "[W 2025-04-01 22:38:54,783] Trial 1 failed with parameters: {'batch_size': 256, 'channels': 3, 'num_conv_layers': 4, 'base_filters': 128, 'kernel_size': 7, 'activation': 'swish', 'num_dense_layers': 1, 'dense_units': 256, 'dropout_rate': 0.3, 'output_activation': 'softmax', 'pool_size': 2} because of the following error: UnknownError().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_574073/1367019713.py\", line 76, in objective\n",
      "    history = model.fit(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    except TypeError as e:\n",
      "tensorflow.python.framework.errors_impl.UnknownError: Graph execution error:\n",
      "\n",
      "Detected at node StatefulPartitionedCall defined at (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "\n",
      "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "\n",
      "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "\n",
      "  File \"/tmp/ipykernel_574073/1367019713.py\", line 105, in <module>\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "\n",
      "  File \"/tmp/ipykernel_574073/1367019713.py\", line 76, in objective\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n",
      "\n",
      "Failed to determine best cudnn convolution algorithm for:\n",
      "%cudnn-conv-bias-activation.14 = (f32[256,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,256,200,200]{3,2,1,0} %bitcast.7454, f32[512,256,7,7]{3,2,1,0} %bitcast.7438, f32[512]{0} %bitcast.7475), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_5_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n",
      "\n",
      "Original error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 20988297216 bytes. [tf-allocator-allocation-error='']\n",
      "\n",
      "To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n",
      "\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_23163]\n",
      "[W 2025-04-01 22:38:54,786] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_574073/1367019713.py\", line 105, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"/tmp/ipykernel_574073/1367019713.py\", line 76, in objective\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nFailed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bias-activation.14 = (f32[256,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,256,200,200]{3,2,1,0} %bitcast.7454, f32[512,256,7,7]{3,2,1,0} %bitcast.7438, f32[512]{0} %bitcast.7475), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_5_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 20988297216 bytes. [tf-allocator-allocation-error='']\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_23163]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 105\u001b[0m\n\u001b[1;32m    101\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# n_trials defines how many different hyperparameter configurations will be evaluated.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Here it's set to 10 for demonstration. In practice, you might increase this number to explore the search space more thoroughly.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m best_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39mvalue, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m10\u001b[39m]\n\u001b[1;32m    108\u001b[0m best_models_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_models\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[10], line 76\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     71\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m load_images_from_paths(val_filenames_tensor, val_labels_tensor,\n\u001b[1;32m     72\u001b[0m                                      channels\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m], ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     73\u001b[0m                                      batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping.\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m  \u001b[38;5;66;03m# Save the trained model to disk.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_574073/1367019713.py\", line 105, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"/tmp/ipykernel_574073/1367019713.py\", line 76, in objective\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nFailed to determine best cudnn convolution algorithm for:\n%cudnn-conv-bias-activation.14 = (f32[256,512,200,200]{3,2,1,0}, u8[0]{0}) custom-call(f32[256,256,200,200]{3,2,1,0} %bitcast.7454, f32[512,256,7,7]{3,2,1,0} %bitcast.7438, f32[512]{0} %bitcast.7475), window={size=7x7 pad=3_3x3_3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBiasActivationForward\", metadata={op_type=\"Conv2D\" op_name=\"functional_1_1/conv2d_5_1/convolution\" source_file=\"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false}\n\nOriginal error: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 20988297216 bytes. [tf-allocator-allocation-error='']\n\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_23163]"
     ]
    }
   ],
   "source": [
    "\n",
    "import gc\n",
    "# Assume these functions are defined elsewhere:\n",
    "# - load_images_from_paths(filenames, labels, channels, ratio, batch_size)\n",
    "# - build_model_from_config(config)\n",
    "#\n",
    "# Also assume that the following dataset variables are defined:\n",
    "# train_filenames_tensor, train_labels_tensor, val_filenames_tensor, val_labels_tensor\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters using Optuna suggestions.\n",
    "    config = {\n",
    "        'batch_size': trial.suggest_categorical('batch_size',[32,64,128,256]),  \n",
    "        'channels': trial.suggest_categorical('channels', [1, 3]),\n",
    "        'num_conv_layers': trial.suggest_categorical('num_conv_layers', [3, 4, 5]),\n",
    "        'base_filters': trial.suggest_categorical('base_filters', [32, 64, 128]),\n",
    "        'kernel_size': trial.suggest_categorical('kernel_size', [3, 5, 7]),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'swish']),\n",
    "        'use_skip': False,\n",
    "        'num_dense_layers': trial.suggest_categorical('num_dense_layers', [1, 2, 3]),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', [128, 256]),\n",
    "        'num_classes': 13,  # Fixed\n",
    "        'dropout_rate': trial.suggest_categorical('dropout_rate', [0.3, 0.5, 0.7]),\n",
    "        'output_activation': trial.suggest_categorical('output_activation', ['softmax', 'sigmoid']),\n",
    "        'pool_size': trial.suggest_categorical('pool_size', [2, 3]),\n",
    "        'task': 'classification',\n",
    "    }\n",
    "    \n",
    "    # Enforce: If use_skip=True, then use_pooling is False; otherwise, True.\n",
    "    config['use_pooling'] = False if config['use_skip'] else True\n",
    "\n",
    "    # Generate a model name from the configuration for logging.\n",
    "    model_name = \"_\".join([\n",
    "        f\"{config['channels']}ch\",\n",
    "        f\"skip_{config['use_skip']}\",\n",
    "        f\"conv{config['num_conv_layers']}\",\n",
    "        f\"k{config['kernel_size']}\",\n",
    "        config['activation'],\n",
    "        f\"dense{config['num_dense_layers']}x{config['dense_units']}\",\n",
    "        f\"drop{config['dropout_rate']}\",\n",
    "        f\"out_{config['output_activation']}\"\n",
    "    ])\n",
    "    trial.set_user_attr('model_name', model_name)\n",
    "# Define model save directory and filename.\n",
    "    model_dir = \"saved_models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_filename = f\"{model_name}.keras\"  # Note: no trial number here.\n",
    "    model_path = os.path.join(model_dir, model_filename)\n",
    "    \n",
    "    # If the model has already been trained, load it and evaluate.\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Model {model_name} already trained. Loading saved model.\")\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        val_dataset = load_images_from_paths(val_filenames_tensor, val_labels_tensor,\n",
    "                                             channels=config['channels'], ratio=1,\n",
    "                                             batch_size=config['batch_size'])\n",
    "        loss, val_acc = model.evaluate(val_dataset, verbose=0)\n",
    "        trial.set_user_attr('model_path', model_path)\n",
    "        return val_acc\n",
    "\n",
    "    # Build the model from the configuration.\n",
    "    model = build_model_from_config(config)\n",
    "    model.compile(\n",
    "        optimizer='lion',\n",
    "        loss='categorical_crossentropy',  # Fixed loss function.\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Load datasets using the specified channels and batch_size.\n",
    "    train_dataset = load_images_from_paths(train_filenames_tensor, train_labels_tensor,\n",
    "                                           channels=config['channels'], ratio=1,\n",
    "                                           batch_size=config['batch_size'])\n",
    "    val_dataset = load_images_from_paths(val_filenames_tensor, val_labels_tensor,\n",
    "                                         channels=config['channels'], ratio=1,\n",
    "                                         batch_size=config['batch_size'])\n",
    "\n",
    "    # Train the model with early stopping.\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,\n",
    "        verbose=0,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "     # Save the trained model to disk.\n",
    "    model_dir = \"saved_models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    model_filename = f\"{model_name}_trial_{trial.number}.keras\"\n",
    "    model_path = os.path.join(model_dir, model_filename)\n",
    "    model.save(model_path)\n",
    "\n",
    "    del train_dataset, val_dataset, model\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    trial.set_user_attr('model_path', model_path)\n",
    "    \n",
    "    val_acc = max(history.history['val_accuracy'])\n",
    "    print(f\"Trial {trial.number} | Model: {model_name} | Val Acc: {val_acc:.4f}\")\n",
    "    gc.collect()\n",
    "    return val_acc\n",
    "    \n",
    "    \n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# n_trials defines how many different hyperparameter configurations will be evaluated.\n",
    "# Here it's set to 10 for demonstration. In practice, you might increase this number to explore the search space more thoroughly.\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_trials = sorted(study.trials, key=lambda t: t.value, reverse=True)[:10]\n",
    "best_models_dir = \"best_models\"\n",
    "os.makedirs(best_models_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nSaving best models:\")\n",
    "for trial in best_trials:\n",
    "    model_path = trial.user_attrs.get('model_path')\n",
    "    model_name = trial.user_attrs.get('model_name')\n",
    "    if model_path and os.path.exists(model_path):\n",
    "        dest_path = os.path.join(best_models_dir, os.path.basename(model_path))\n",
    "        shutil.copy(model_path, dest_path)\n",
    "        print(f\"Saved {model_name} with Val Acc: {trial.value:.4f} to {dest_path}\")\n",
    "\n",
    "# Display the best trial's results.\n",
    "best_trial = study.best_trial\n",
    "print(\"\\nBest trial:\")\n",
    "print(f\"  Model: {best_trial.user_attrs['model_name']}\")\n",
    "print(f\"  Validation Accuracy: {best_trial.value:.4f}\")\n",
    "print(\"  Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec404ce8136a4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-01T16:27:47.600090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cast (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,924</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,729</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cast (\u001b[38;5;33mCast\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m132\u001b[0m)            │        \u001b[38;5;34m33,924\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │         \u001b[38;5;34m1,729\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">408,261</span> (1.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m408,261\u001b[0m (1.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,365</span> (1.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m407,365\u001b[0m (1.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 17:27:48.351644: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 117s - 1s/step - accuracy: 0.2073 - loss: 2.4318 - val_accuracy: 0.1706 - val_loss: 2.6611\n",
      "Epoch 2/50\n"
     ]
    }
   ],
   "source": [
    "# model_dict = {}\n",
    "# model_dir = \"saved_models\"\n",
    "# os.makedirs(model_dir, exist_ok=True)\n",
    "# paths_pre_train_model_dict = {}\n",
    "# \n",
    "# \n",
    "# for i, config in enumerate(valid_configs):\n",
    "#     # Generate unique model name\n",
    "#     name_parts = [\n",
    "#         f\"{config['channels']}ch\",\n",
    "#         f\"skip_{config['use_skip']}\",\n",
    "#         f\"conv{config['num_conv_layers']}\",\n",
    "#         f\"k{config['kernel_size']}\",\n",
    "#         config['activation'],\n",
    "#         f\"dense{config['num_dense_layers']}x{config['dense_units']}\",\n",
    "#         f\"drop{config['dropout_rate']}\",\n",
    "#         f\"out_{config['output_activation']}\"\n",
    "#     ]\n",
    "#     \n",
    "#     model_name = \"_\".join(name_parts)\n",
    "#     # model_filename = model_name + \".keras\"\n",
    "#     # model_path = os.path.join(model_dir, model_filename)\n",
    "# \n",
    "#     # Build and compile\n",
    "#     model = build_model_from_config(config)\n",
    "#     model.compile(\n",
    "#         optimizer='adam',\n",
    "#         loss='categorical_crossentropy',  # Fixed loss function\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "#     # model.save(model_path,overwrite=True)\n",
    "#     # model.summary()\n",
    "#     # paths_pre_train_model_dict[model_name] = {\n",
    "#     #     'model_path': model_path,\n",
    "#     #     'config': config\n",
    "#     # }\n",
    "#     # \n",
    "#     # # Train\n",
    "#     history = model.fit(\n",
    "#         train_dataset,  # Dataset yields (images, targets)\n",
    "#         validation_data=val_dataset,\n",
    "#         epochs=50,\n",
    "#         batch_size=batch_size,\n",
    "#         verbose=2,\n",
    "#         callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)]\n",
    "#     )\n",
    "# \n",
    "#     # Store results\n",
    "#     model_dict[model_name] = {\n",
    "#         'model': model,\n",
    "#         'history': history.history,\n",
    "#         'config': config\n",
    "#     }\n",
    "#     print(f\"Trained {model_name} | Val acc: {max(history.history['val_accuracy']):.4f}\")\n",
    "#     # print(f\"Created {model_name} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabfb41fdac8151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['1ch_skip_False_conv3_k3_relu_dense1x132_out_softmax']['history']['accuracy']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
