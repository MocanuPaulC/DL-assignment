{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:08.675821Z",
     "start_time": "2025-04-01T00:00:05.889003Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "import importlib\n",
    "import common_utils\n",
    "importlib.reload(common_utils)\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "from common_utils import get_unique_image_shapes,get_unique_image_paths,load_images_from_paths,build_image_dataframe,split_data, bin_ages, build_cnn_model,build_model_from_config"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 02:00:06.573427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 02:00:06.688427: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743465606.733646   76914 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743465606.747487   76914 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743465606.855243   76914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743465606.855265   76914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743465606.855268   76914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743465606.855269   76914 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-01 02:00:06.868314: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:08.954843Z",
     "start_time": "2025-04-01T00:00:08.680033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ],
   "id": "b449a3fa11f15cff",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:09.060724Z",
     "start_time": "2025-04-01T00:00:09.046668Z"
    }
   },
   "cell_type": "code",
   "source": "image_paths_csv = pd.read_csv('./processed_data/image_paths.csv')",
   "id": "a5acada596262b76",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:09.069881Z",
     "start_time": "2025-04-01T00:00:09.066159Z"
    }
   },
   "cell_type": "code",
   "source": "paths_train_df, paths_val_df, paths_test_df = split_data(image_paths_csv)",
   "id": "e6164dda010f022",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:09.112778Z",
     "start_time": "2025-04-01T00:00:09.109432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "configurations = {\n",
    "    'channels': [3],\n",
    "    'use_skip': [True, False],\n",
    "    'num_conv_layers': [3, 4,],\n",
    "    'base_filters': [16,32, 64],\n",
    "    'kernel_size': [3, 5,7],\n",
    "    'activation': ['relu', 'elu','leakyrelu','tanh','swish'],\n",
    "    'num_dense_layers': [1, 2,3,4],\n",
    "    'dense_units': [128, 256],\n",
    "    'dropout_rate': [0.3, 0.5,0.7],\n",
    "    'output_activation': ['sigmoid', 'softmax'],\n",
    "}\n"
   ],
   "id": "f77f27c3567505cf",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:09.177601Z",
     "start_time": "2025-04-01T00:00:09.156347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "all_combinations = list(itertools.product(*configurations.values()))\n",
    "valid_configs = []\n",
    "\n",
    "for combo in all_combinations:\n",
    "    params = dict(zip(configurations.keys(), combo))\n",
    "    \n",
    "    # Enforce: If use_skip=True, use_pooling=False\n",
    "    if params['use_skip']:\n",
    "        params['use_pooling'] = False\n",
    "    else:\n",
    "        params['use_pooling'] = True  # Or add to search space\n",
    "    \n",
    "    valid_configs.append(params)\n",
    "\n"
   ],
   "id": "e736c21984667af3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:09.407869Z",
     "start_time": "2025-04-01T00:00:09.227450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = load_images_from_paths(paths_train_df, channels=3,ratio=0.1)\n",
    "val_dataset = load_images_from_paths(paths_val_df, channels=3,ratio=0.1)\n"
   ],
   "id": "c148d5e8b1c8ddb9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743465609.347444   76914 gpu_process_state.cc:208] Using CUDA malloc Async allocator for GPU: 0\n",
      "I0000 00:00:1743465609.349005   76914 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6172 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:09.420466Z",
     "start_time": "2025-04-01T00:00:09.418431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_dataset(dataset, shuffle=False):\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)  # Adjust buffer size as needed\n",
    "    dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ],
   "id": "fc46df10f99cd492",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:09.483586Z",
     "start_time": "2025-04-01T00:00:09.465879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = configure_dataset(train_dataset, shuffle=True)\n",
    "val_dataset = configure_dataset(val_dataset)\n"
   ],
   "id": "b9aa26f54d02a548",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:09.523995Z",
     "start_time": "2025-04-01T00:00:09.521052Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8c6cd364482f42a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:00:50.427304Z",
     "start_time": "2025-04-01T00:00:09.577883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dict = {}\n",
    "\n",
    "for i, config in enumerate(valid_configs):  # Limit to 10 for demo\n",
    "    # Generate unique model name (your existing code)\n",
    "    name_parts = [\n",
    "        f\"{config['channels']}ch\",\n",
    "        f\"skip_{config['use_skip']}\",\n",
    "        f\"conv{config['num_conv_layers']}\",\n",
    "        f\"k{config['kernel_size']}\",\n",
    "        config['activation'],\n",
    "        f\"dense{config['num_dense_layers']}x{config['dense_units']}\",\n",
    "        f\"drop{config['dropout_rate']}\",\n",
    "        f\"out_{config['output_activation']}\"\n",
    "    ]\n",
    "    model_name = \"_\".join(name_parts)\n",
    "    \n",
    "    # Build and compile\n",
    "    model = build_model_from_config(config)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',  # Fixed loss function\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    \n",
    "        # Train\n",
    "    history = model.fit(\n",
    "        train_dataset,  # Dataset yields (images, targets)\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,\n",
    "        batch_size=8,\n",
    "        verbose=0,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    model_dict[model_name] = {\n",
    "        'model': model,\n",
    "        'history': history.history,\n",
    "        'config': config\n",
    "    }\n",
    "    print(f\"Trained {model_name} | Val acc: {max(history.history['val_accuracy']):.4f}\")\n"
   ],
   "id": "83ec404ce8136a4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743465612.488343   77034 service.cc:152] XLA service 0x75c830015680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743465612.488362   77034 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6\n",
      "2025-04-01 02:00:12.532293: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743465612.908533   77034 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-04-01 02:00:19.536887: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=4} for conv %cudnn-conv-bw-filter.10 = (f16[64,3,3,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,200,200,32]{3,2,1,0} %bitcast.9204, f16[32,200,200,64]{3,2,1,0} %bitcast.9166), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 02:00:20.056140: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.519271616s\n",
      "Trying algorithm eng19{k2=4} for conv %cudnn-conv-bw-filter.10 = (f16[64,3,3,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,200,200,32]{3,2,1,0} %bitcast.9204, f16[32,200,200,64]{3,2,1,0} %bitcast.9166), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 02:00:21.056475: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=3} for conv %cudnn-conv-bw-filter.10 = (f16[64,3,3,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,200,200,32]{3,2,1,0} %bitcast.9204, f16[32,200,200,64]{3,2,1,0} %bitcast.9166), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 02:00:21.629747: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.573495614s\n",
      "Trying algorithm eng19{k2=3} for conv %cudnn-conv-bw-filter.10 = (f16[64,3,3,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,200,200,32]{3,2,1,0} %bitcast.9204, f16[32,200,200,64]{3,2,1,0} %bitcast.9166), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 02:00:22.629986: E external/local_xla/xla/service/slow_operation_alarm.cc:73] Trying algorithm eng19{k2=2} for conv %cudnn-conv-bw-filter.10 = (f16[64,3,3,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,200,200,32]{3,2,1,0} %bitcast.9204, f16[32,200,200,64]{3,2,1,0} %bitcast.9166), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 02:00:23.197417: E external/local_xla/xla/service/slow_operation_alarm.cc:140] The operation took 1.567605479s\n",
      "Trying algorithm eng19{k2=2} for conv %cudnn-conv-bw-filter.10 = (f16[64,3,3,32]{3,2,1,0}, u8[0]{0}) custom-call(f16[32,200,200,32]{3,2,1,0} %bitcast.9204, f16[32,200,200,64]{3,2,1,0} %bitcast.9166), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_o01i->b01f, custom_call_target=\"__cudnn$convBackwardFilter\", metadata={op_type=\"Conv2DBackpropFilter\" op_name=\"gradient_tape/functional_1/conv2d_4_1/convolution/Conv2DBackpropFilter\" source_file=\"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\" source_line=1200}, backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2025-04-01 02:00:23.866385: W external/local_xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 2.35GiB (2518684013 bytes) by rematerialization; only reduced to 4.97GiB (5337079472 bytes), down from 4.97GiB (5337080709 bytes) originally\n",
      "I0000 00:00:1743465625.585326   77034 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-04-01 02:00:33.545076: W external/local_xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 2.35GiB (2525524127 bytes) by rematerialization; only reduced to 4.71GiB (5059569400 bytes), down from 4.71GiB (5059558662 bytes) originally\n",
      "2025-04-01 02:00:35.673431: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_150_0', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-04-01 02:00:35.768737: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_150', 244 bytes spill stores, 244 bytes spill loads\n",
      "\n",
      "2025-04-01 02:00:38.310198: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_150', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 3ch_skip_True_conv3_k3_relu_dense1x128_drop0.3_out_sigmoid | Val acc: 0.2267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 02:00:48.253569: W external/local_xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 2.35GiB (2518684013 bytes) by rematerialization; only reduced to 4.97GiB (5337079472 bytes), down from 4.97GiB (5337080709 bytes) originally\n",
      "2025-04-01 02:00:49.948355: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:359] gpu_async_0 cuMemAllocAsync failed to allocate 1404590000 bytes: RESOURCE_EXHAUSTED: : CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      " Reported by CUDA: Free memory/Total memory: 1403322368/8248885248\n",
      "2025-04-01 02:00:49.948575: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:364] Stats: Limit:                      6472138752\n",
      "InUse:                      6569536193\n",
      "MaxInUse:                   7864776049\n",
      "NumAllocs:                        3040\n",
      "MaxAllocSize:               1404590000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-04-01 02:00:49.948596: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:68] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\n",
      "2025-04-01 02:00:49.948599: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 1, 1\n",
      "2025-04-01 02:00:49.948602: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 4, 34\n",
      "2025-04-01 02:00:49.948604: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 8, 11\n",
      "2025-04-01 02:00:49.948605: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 16, 2\n",
      "2025-04-01 02:00:49.948607: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 52, 5\n",
      "2025-04-01 02:00:49.948609: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 64, 21\n",
      "2025-04-01 02:00:49.948611: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 128, 21\n",
      "2025-04-01 02:00:49.948613: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 192, 5\n",
      "2025-04-01 02:00:49.948614: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 256, 22\n",
      "2025-04-01 02:00:49.948616: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 512, 5\n",
      "2025-04-01 02:00:49.948618: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 656, 1\n",
      "2025-04-01 02:00:49.948620: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 1028, 1\n",
      "2025-04-01 02:00:49.948621: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 1376, 1\n",
      "2025-04-01 02:00:49.948623: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 1728, 5\n",
      "2025-04-01 02:00:49.948625: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 2048, 5\n",
      "2025-04-01 02:00:49.948627: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 5512, 1\n",
      "2025-04-01 02:00:49.948628: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 6656, 5\n",
      "2025-04-01 02:00:49.948630: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 8192, 5\n",
      "2025-04-01 02:00:49.948632: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 18432, 5\n",
      "2025-04-01 02:00:49.948634: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 73728, 5\n",
      "2025-04-01 02:00:49.948636: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 15360000, 1\n",
      "2025-04-01 02:00:49.948637: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:71] 1310720000, 5\n",
      "2025-04-01 02:00:49.948648: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:104] CU_MEMPOOL_ATTR_RESERVED_MEM_CURRENT: 6643777536\n",
      "2025-04-01 02:00:49.948651: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:106] CU_MEMPOOL_ATTR_USED_MEM_CURRENT: 6569536193\n",
      "2025-04-01 02:00:49.948653: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:107] CU_MEMPOOL_ATTR_RESERVED_MEM_HIGH: 8019509248\n",
      "2025-04-01 02:00:49.948655: E external/local_xla/xla/stream_executor/gpu/gpu_cudamallocasync_allocator.cc:108] CU_MEMPOOL_ATTR_USED_MEM_HIGH: 7864776049\n",
      "2025-04-01 02:00:49.949372: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 1404590000 bytes.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
      " [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3047, in run_cell\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3102, in _run_cell\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3489, in run_ast_nodes\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n\n  File \"/tmp/ipykernel_76914/2125581266.py\", line 27, in <module>\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1404590000 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_11391]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mResourceExhaustedError\u001B[39m                    Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     19\u001B[39m model.compile(\n\u001B[32m     20\u001B[39m     optimizer=\u001B[33m'\u001B[39m\u001B[33madam\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m     21\u001B[39m     loss=\u001B[33m'\u001B[39m\u001B[33msparse_categorical_crossentropy\u001B[39m\u001B[33m'\u001B[39m,  \u001B[38;5;66;03m# Fixed loss function\u001B[39;00m\n\u001B[32m     22\u001B[39m     metrics=[\u001B[33m'\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     23\u001B[39m )\n\u001B[32m     26\u001B[39m     \u001B[38;5;66;03m# Train\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m history = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Dataset yields (images, targets)\u001B[39;49;00m\n\u001B[32m     29\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mkeras\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEarlyStopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# Store results\u001B[39;00m\n\u001B[32m     37\u001B[39m model_dict[model_name] = {\n\u001B[32m     38\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m'\u001B[39m: model,\n\u001B[32m     39\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mhistory\u001B[39m\u001B[33m'\u001B[39m: history.history,\n\u001B[32m     40\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mconfig\u001B[39m\u001B[33m'\u001B[39m: config\n\u001B[32m     41\u001B[39m }\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001B[39m, in \u001B[36mquick_execute\u001B[39m\u001B[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     52\u001B[39m   ctx.ensure_initialized()\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[32m     54\u001B[39m                                       inputs, attrs, num_outputs)\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m core._NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     56\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mResourceExhaustedError\u001B[39m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3047, in run_cell\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3102, in _run_cell\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3489, in run_ast_nodes\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n\n  File \"/tmp/ipykernel_76914/2125581266.py\", line 27, in <module>\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/home/paul/PycharmProjects/DL-assignment/.venv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1404590000 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_11391]"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
