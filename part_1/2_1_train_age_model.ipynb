{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmgbfqAKOsxR"
   },
   "source": [
    "# Plan of Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxEzjKuOPr5n"
   },
   "source": [
    "**Our age prediction CNN model shall be defined and trained by**:\n",
    "1. Importing **training and test datasets** from Google Drive Input Sub-folder\n",
    "2. **Training dataset is already augmented** and has 234,000 images\n",
    "3. **Greyscaling images** instead of using RGB color images\n",
    "4. Defining our intuitively **distributed classes of age-ranges**\n",
    "5. Using **60 epochs** on our **optimized CNN Architecture**, comprising of:\n",
    "    - an input *Conv2D* layer (with 32 filters) paired with an *AveragePooling2D* layer,\n",
    "    - 3 pairs of *Conv2D* (with 64, 128 & 256 filters) and *AveragePooling2D* layers,\n",
    "    - a *GlobalAveragePooling2D* layer,\n",
    "    - 1 *Dense* layer with 132 nodes, and\n",
    "    - an output *Dense* layer with 7 nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mount Google Drive & Imports"
   ],
   "metadata": {
    "id": "io9rnPUceyKx"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ndBDuhivUu7C",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "db0c3318-eb11-4fb3-a720-45414585010e",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:34.942825Z",
     "start_time": "2025-04-02T09:24:34.941201Z"
    }
   },
   "source": [
    "#@title Mount Google Drive {display-mode: \"form\"}\n",
    "\n",
    "# This code will be hidden when the notebook is loaded.\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_kTFrNNPU7wP",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.006696Z",
     "start_time": "2025-04-02T09:24:35.003959Z"
    }
   },
   "source": [
    "import io\n",
    "\n",
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common_utils import *\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, AveragePooling2D, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "from tensorflow.python.keras import utils\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "# Setting random seeds to reduce the amount of randomness in the neural net weights and results\n",
    "# The results may still not be exactly reproducible\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ggkatHAQU75X",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8dbff126-b83b-4f9f-983b-0cfdbd2ab7f0",
    "cellView": "form",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.010258Z",
     "start_time": "2025-04-02T09:24:35.008213Z"
    }
   },
   "source": [
    "#@title Check for GPU\n",
    "\n",
    "# Testing to ensure GPU is being utilized\n",
    "# Ensure that the Runtime Type for this notebook is set to GPU\n",
    "# If a GPU device is not found, change the runtime type under: Runtime>> Change runtime type>> Hardware accelerator>> GPU\n",
    "# and run the notebook from the beginning again.\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 10:24:35.008863: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-02 10:24:35.008876: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation"
   ],
   "metadata": {
    "id": "xmhd2RT_fIaS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Dataset"
   ],
   "metadata": {
    "id": "5P0vu49KbWXH"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tCN57I6AU7_g",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d1539ac1-8bd3-4d87-c11a-37f01f066e60",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.012305Z",
     "start_time": "2025-04-02T09:24:35.011041Z"
    }
   },
   "source": [
    "# Unzipping the dataset file combined_faces.zip\n",
    "\n",
    "# combined_faces_zip_path = \"/content/drive/My Drive/1_LiveProjects/Project5_AgeGenderEmotion_Detection/1.1_age_input_output/input/combined_faces.zip\"\n",
    "\n",
    "# with ZipFile(combined_faces_zip_path, 'r') as myzip:\n",
    "#     myzip.extractall()\n",
    "#     print('Done unzipping combined_faces.zip')"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q61d9-wxU8Ea",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9f8a6897-92ba-4594-96da-c3d3747264d1",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.014442Z",
     "start_time": "2025-04-02T09:24:35.012973Z"
    }
   },
   "source": [
    "# Unzipping the dataset file combined_faces.zip\n",
    "\n",
    "# combined_faces_zip_path = \"/content/drive/My Drive/1_LiveProjects/Project5_AgeGenderEmotion_Detection/1.1_age_input_output/input/combined_faces_train_augmented.zip\"\n",
    "# \n",
    "# with ZipFile(combined_faces_zip_path, 'r') as myzip:\n",
    "#     myzip.extractall()\n",
    "#     print('Done unzipping combined_faces_train_augmented.zip')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yu50xj5jU8I6",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.032267Z",
     "start_time": "2025-04-02T09:24:35.024063Z"
    }
   },
   "source": [
    "# Importing the augmented training dataset and testing dataset to create tensors of images using the filename paths.\n",
    "\n",
    "image_paths_csv = pd.read_csv(\"./processed_data/image_paths.csv\")\n",
    "paths_train_df, paths_val_df, paths_test_df = split_data(image_paths_csv)\n",
    "\n",
    "\n",
    "# test_df = pd.read_csv(\"/content/drive/My Drive/1_LiveProjects/Project5_AgeGenderEmotion_Detection/1.1_age_input_output/input/images_filenames_labels_test.csv\")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Organize Dataset"
   ],
   "metadata": {
    "id": "s52pb794dHhn"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sJSY34ShavL_",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.046531Z",
     "start_time": "2025-04-02T09:24:35.044178Z"
    }
   },
   "source": [
    "# Converting the filenames and target class labels into lists for augmented train and test datasets.\n",
    "\n",
    "paths_train_df_list = list(paths_train_df['path'])\n",
    "paths_train_labels_list = list(paths_train_df['age_bin'])\n",
    "\n",
    "\n",
    "paths_test_df_list = list(paths_test_df['path'])\n",
    "paths_test_labels_list = list(paths_test_df['age_bin'])\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S7ye8s5savP5",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.056816Z",
     "start_time": "2025-04-02T09:24:35.047981Z"
    }
   },
   "source": [
    "# Creating tensorflow constants of filenames and labels for augmented train and test datasets from the lists defined above.\n",
    "\n",
    "train_aug_filenames_tensor = tf.constant(paths_train_df_list)\n",
    "train_aug_labels_tensor = tf.constant(paths_train_labels_list)\n",
    "\n",
    "test_filenames_tensor = tf.constant(paths_test_df_list)\n",
    "test_labels_tensor = tf.constant(paths_test_labels_list)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 10:24:35.048814: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-04-02 10:24:35.048853: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image Greyscale Function"
   ],
   "metadata": {
    "id": "FIUoR5YidZMA"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4ALPko3Xavbo",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.059181Z",
     "start_time": "2025-04-02T09:24:35.057576Z"
    }
   },
   "source": [
    "# Defining a function to read the image, decode the image from given tensor and one-hot encode the image label class.\n",
    "# Changing the channels para in tf.io.decode_jpeg from 3 to 1 changes the output images from RGB coloured to grayscale.\n",
    "\n",
    "num_classes = 13\n",
    "\n",
    "def _parse_function(filename, label):\n",
    "\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.io.decode_jpeg(image_string, channels=1)    # channels=1 to convert to grayscale, channels=3 to convert to RGB.\n",
    "    # image_resized = tf.image.resize(image_decoded, [200, 200])\n",
    "    label = tf.one_hot(label, num_classes)\n",
    "\n",
    "    return image_decoded, label"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OTyXZ2tUavZh",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.095976Z",
     "start_time": "2025-04-02T09:24:35.062185Z"
    }
   },
   "source": [
    "# Getting the dataset ready for the neural network.\n",
    "# Using the tensor vectors defined above, accessing the images in the dataset and passing them through the function defined above.\n",
    "\n",
    "train_aug_dataset = tf.data.Dataset.from_tensor_slices((train_aug_filenames_tensor, train_aug_labels_tensor))\n",
    "train_aug_dataset = train_aug_dataset.map(_parse_function)\n",
    "# train_aug_dataset = train_aug_dataset.repeat(3)\n",
    "train_aug_dataset = train_aug_dataset.batch(512)    # Same as batch_size hyperparameter in model.fit() below.\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_filenames_tensor, test_labels_tensor))\n",
    "test_dataset = test_dataset.map(_parse_function)\n",
    "# test_dataset = test_dataset.repeat(3)\n",
    "test_dataset = test_dataset.batch(512)    # Same as batch_size hyperparameter in model.fit() below."
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN Architecture"
   ],
   "metadata": {
    "id": "TSLahA9adp-o"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kcW44JspavWb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "200d9ea4-2b9d-43ed-ab91-7f649182a40e",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.314968Z",
     "start_time": "2025-04-02T09:24:35.096937Z"
    }
   },
   "source": [
    "# Defining the architecture of the sequential neural network.\n",
    "\n",
    "model=build_cnn_model(\n",
    "    channels=1,\n",
    "    dropout_rate=0,\n",
    "    task=\"classification\",\n",
    "    num_classes=13,\n",
    "    num_conv_layers=4,\n",
    "    conv_filters=[32, 64, 128, 256],\n",
    "    kernel_size=3,\n",
    "    activation=\"relu\",\n",
    "    num_dense_layers=1,\n",
    "    dense_units=[132],\n",
    "    output_activation=\"softmax\"\n",
    ")\n",
    "\n",
    "final_cnn = Sequential([\n",
    "    Input(shape=(200, 200, 1)),\n",
    "    Conv2D(filters=32, kernel_size=3, activation='relu'),\n",
    "    AveragePooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(filters=64, kernel_size=3, activation='relu'),\n",
    "    AveragePooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(filters=128, kernel_size=3, activation='relu'),\n",
    "    AveragePooling2D(pool_size=(2,2)),\n",
    "\n",
    "    Conv2D(filters=256, kernel_size=3, activation='relu'),\n",
    "    AveragePooling2D(pool_size=(2,2)),\n",
    "\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(132, activation='relu'),\n",
    "    Dense(13, activation='softmax')\n",
    "])\n",
    "final_cnn.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m198\u001B[0m, \u001B[38;5;34m198\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │           \u001B[38;5;34m320\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m99\u001B[0m, \u001B[38;5;34m99\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mAveragePooling2D\u001B[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m97\u001B[0m, \u001B[38;5;34m97\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │        \u001B[38;5;34m18,496\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m48\u001B[0m, \u001B[38;5;34m48\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mAveragePooling2D\u001B[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m46\u001B[0m, \u001B[38;5;34m46\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │        \u001B[38;5;34m73,856\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_2             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m23\u001B[0m, \u001B[38;5;34m23\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mAveragePooling2D\u001B[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m21\u001B[0m, \u001B[38;5;34m21\u001B[0m, \u001B[38;5;34m256\u001B[0m)    │       \u001B[38;5;34m295,168\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_3             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m, \u001B[38;5;34m10\u001B[0m, \u001B[38;5;34m256\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mAveragePooling2D\u001B[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m132\u001B[0m)            │        \u001B[38;5;34m33,924\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m13\u001B[0m)             │         \u001B[38;5;34m1,729\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_3             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,924</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,729</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m423,493\u001B[0m (1.62 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">423,493</span> (1.62 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m423,493\u001B[0m (1.62 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">423,493</span> (1.62 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.330708Z",
     "start_time": "2025-04-02T09:24:35.315533Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001B[38;5;33mInputLayer\u001B[0m)        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m1\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001B[38;5;33mConv2D\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │           \u001B[38;5;34m320\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │           \u001B[38;5;34m128\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │        \u001B[38;5;34m18,496\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m64\u001B[0m)   │           \u001B[38;5;34m256\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │        \u001B[38;5;34m73,856\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m128\u001B[0m)  │           \u001B[38;5;34m512\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m256\u001B[0m)  │       \u001B[38;5;34m295,168\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m200\u001B[0m, \u001B[38;5;34m256\u001B[0m)  │         \u001B[38;5;34m1,024\u001B[0m │\n",
       "│ (\u001B[38;5;33mBatchNormalization\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m132\u001B[0m)            │        \u001B[38;5;34m33,924\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m13\u001B[0m)             │         \u001B[38;5;34m1,729\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,924</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,729</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m425,413\u001B[0m (1.62 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">425,413</span> (1.62 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m424,453\u001B[0m (1.62 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">424,453</span> (1.62 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m960\u001B[0m (3.75 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">960</span> (3.75 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uBG5MY8zavTn",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.363310Z",
     "start_time": "2025-04-02T09:24:35.335796Z"
    }
   },
   "source": [
    "# Compiling the above created CNN architecture.\n",
    "\n",
    "\n",
    "model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',  # Fixed loss function.\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "final_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nWy7fRfHfu8L",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.366174Z",
     "start_time": "2025-04-02T09:24:35.364256Z"
    }
   },
   "source": [
    "# Creating a TensorBoard callback object and saving it at the desired location.\n",
    "\n",
    "# tensorboard = TensorBoard(log_dir=f\"/content/drive/My Drive/1_LiveProjects/Project5_AgeGenderEmotion_Detection/1.1_age_input_output/output/cnn_logs\")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pKfqLLbS3ld"
   },
   "source": [
    "We shall also use ***ModelCheckpoint*** as a callback while training the final CNN model so as to be able to save the model as it continues training and improving in performance over 60 epochs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X6dQDDhmfuzo",
    "ExecuteTime": {
     "end_time": "2025-04-02T09:24:35.369589Z",
     "start_time": "2025-04-02T09:24:35.367604Z"
    }
   },
   "source": [
    "# Creating a ModelCheckpoint callback object to save the model according to the value of val_accuracy.\n",
    "\n",
    "# checkpoint = ModelCheckpoint(filepath=f\"/content/drive/My Drive/1_LiveProjects/Project5_AgeGenderEmotion_Detection/1.1_age_input_output/output/cnn_logs/age_model_checkpoint.h5\",\n",
    "#                              monitor='val_accuracy',\n",
    "#                              save_best_only=True,\n",
    "#                              save_weights_only=False,\n",
    "#                              verbose=1\n",
    "#                             )"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Fitting"
   ],
   "metadata": {
    "id": "jxVlNLLMeLWw"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hU4LJT6xYjJp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "29700e2d-b475-4995-d6b9-25cbe193b5c7",
    "ExecuteTime": {
     "start_time": "2025-04-02T09:24:35.370067Z"
    }
   },
   "source": [
    "# Fitting the above created CNN model.\n",
    "\n",
    "final_cnn_history = final_cnn.fit(train_aug_dataset,\n",
    "                                  batch_size=512,\n",
    "                                  validation_data=test_dataset,\n",
    "                                  epochs=60,\n",
    "                                  verbose=2,\n",
    "                                  # callbacks=[tensorboard, checkpoint],\n",
    "                                  shuffle=False    # shuffle=False to reduce randomness and increase reproducibility\n",
    "                                 )\n",
    "\n",
    "\n",
    "history=model.fit(\n",
    "        train_aug_dataset,\n",
    "        validation_data=test_dataset,\n",
    "        epochs=50,\n",
    "        verbose=2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "    )\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-02 10:24:35.668841: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 31s - 2s/step - accuracy: 0.0908 - loss: 12.1108 - val_accuracy: 0.1578 - val_loss: 3.7527\n",
      "Epoch 2/60\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking Model Performance"
   ],
   "metadata": {
    "id": "siwy6UeOavHB"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RP7vxY_6YjCM"
   },
   "source": [
    "# Checking the train and test loss and accuracy values from the neural network above.\n",
    "\n",
    "train_loss = final_cnn_history.history['loss']\n",
    "test_loss = final_cnn_history.history['val_loss']\n",
    "train_accuracy = final_cnn_history.history['accuracy']\n",
    "test_accuracy = final_cnn_history.history['val_accuracy']\n",
    "\n",
    "train_loss_func = model.history['loss']\n",
    "test_loss_func = model.history['val_loss']\n",
    "train_accuracy_func = model.history['accuracy']\n",
    "test_accuracy_func = model.history['val_accuracy']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "COkki9JUU8kf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "outputId": "fd227df4-734f-41cd-979b-36ac79375bfb"
   },
   "source": [
    "# Plotting a line chart to visualize the loss and accuracy values by epochs.\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,7))\n",
    "\n",
    "ax = ax.ravel()\n",
    "\n",
    "ax[0].plot(train_loss, label='Train Loss', color='royalblue', marker='o', markersize=5)\n",
    "ax[0].plot(test_loss, label='Test Loss', color = 'orangered', marker='o', markersize=5)\n",
    "\n",
    "ax[0].set_xlabel('Epochs', fontsize=14)\n",
    "ax[0].set_ylabel('Categorical Crossentropy', fontsize=14)\n",
    "\n",
    "ax[0].legend(fontsize=14)\n",
    "ax[0].tick_params(axis='both', labelsize=12)\n",
    "\n",
    "ax[1].plot(train_accuracy, label='Train Accuracy', color='royalblue', marker='o', markersize=5)\n",
    "ax[1].plot(test_accuracy, label='Test Accuracy', color='orangered', marker='o', markersize=5)\n",
    "ax[1].plot(train_accuracy_func, label='Train Accuracy Func', color='green', marker='o', markersize=5)\n",
    "ax[1].plot(test_accuracy_func, label='Test Accuracy Func', color='purple', marker='o', markersize=5)\n",
    "\n",
    "ax[1].set_xlabel('Epochs', fontsize=14)\n",
    "ax[1].set_ylabel('Accuracy', fontsize=14)\n",
    "\n",
    "ax[1].legend(fontsize=14)\n",
    "ax[1].tick_params(axis='both', labelsize=12)\n",
    "\n",
    "fig.suptitle(x=0.5, y=0.92, t=\"Lineplots showing loss and accuracy of CNN model by epochs\", fontsize=16)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F75NJtVJU8oD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ea038f01-1c6b-415f-cfb8-0682d98c46b1"
   },
   "source": [
    "\n",
    "final_cnn_score = final_cnn.evaluate(test_dataset, verbose=1)\n",
    "model_score = model.evaluate(test_dataset, verbose=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h5JdZYFMU8s0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0f1c3605-8bfa-4bdb-ff09-2d7fd847bb82"
   },
   "source": [
    "# Printing the relevant score summary.\n",
    "\n",
    "final_cnn_labels = final_cnn.metrics_names\n",
    "print(f'CNN model {final_cnn_labels[0]} \\t\\t= {round(final_cnn_score[0], 3)}')\n",
    "print(f'CNN model {final_cnn_labels[1]} \\t= {round(final_cnn_score[1], 3)}')\n",
    "\n",
    "model_labels = model.metrics_names\n",
    "print(f'Func model {model_labels[0]} \\t= {round(model_score[0], 3)}')\n",
    "print(f'Func model {model_labels[1]} \\t= {round(model_score[1], 3)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fatkGFAkrjCF"
   },
   "source": "# Saving the model as a h5 file for possible use later.",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plotting Confusion Matrix"
   ],
   "metadata": {
    "id": "EpFNy9QFe_pN"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R1e1uYqHU8wx"
   },
   "source": [
    "# Generating predictions from the model above.\n",
    "\n",
    "final_cnn_pred = final_cnn.predict(test_dataset)\n",
    "final_cnn_pred = final_cnn_pred.argmax(axis=-1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T6hAs7SlU9Zi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fb5f47fe-011c-44ec-ddb2-4c79c49d9e05"
   },
   "source": [
    "# Generating a confusion matrix based on above predictions.\n",
    "\n",
    "conf_mat = confusion_matrix(paths_test_labels_list, final_cnn_pred)\n",
    "conf_mat"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "conf_mat_func = confusion_matrix(paths_test_labels_list, model.predict(test_dataset).argmax(axis=-1))\n",
    "conf_mat_func"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uACH8n6fU95F"
   },
   "source": [
    "# Defining a function to plot the confusion matrix in a grid for easier visualization.\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', export_as='confusion_matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True labels', fontsize=14)\n",
    "    plt.xlabel('Predicted labels', fontsize=14)\n",
    "\n",
    "    # Exporting plot image in PNG format.\n",
    "    plt.savefig(f'/content/drive/My Drive/1_LiveProjects/Project5_AgeGenderEmotion_Detection/1.1_age_input_output/output/cnn_logs/{export_as}.png', bbox_inches='tight');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J1XROLD_U91Z",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "outputId": "33361272-b755-4ac1-c8b5-611a8077f9d3"
   },
   "source": [
    "# Plotting the confusion matrix using the function defined above.\n",
    "\n",
    "cm_plot_labels = ['1-2', '3-5', '6-8', '9-12', '13-17', '18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-84', '85+']\n",
    "plt.figure(figsize=(16,8))\n",
    "plot_confusion_matrix(conf_mat, cm_plot_labels, normalize=True,\n",
    "                      title=\"Confusion Matrix based on predictions from CNN model\",\n",
    "                      export_as=\"final_cnn_conf_mat_norm\"\n",
    "                     )\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cpS3994e9GM"
   },
   "source": [
    "Below is the **final summary of age prediction CNN model**.\n",
    "\n",
    "\n",
    "As with any data science workflow, the deep learning approach presented above does have its own limitations as well. For instance, the original datasets used in this project only had about 33,000 images when combined together. Even though the training dataset was augmented to increase it's size from 23,440 images to 234,400 images, there is always a possibility that an **even larger training dataset with more variation in the images would have resulted in even better results**.\n",
    "\n",
    "Another approach to this project could be to use **Transfer Learning (using the architecture and layer weights from a pre-trained neural network)** instead of creating and training a neural network from scratch.\n",
    "\n",
    "Ref: https://towardsdatascience.com/age-detection-using-facial-images-traditional-machine-learning-vs-deep-learning-2437b2feeab2 by Prerak Agarwal"
   ]
  }
 ]
}
